{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块一：Tensor 的创建与属性 ✨\n",
    "这部分将带您熟悉创建张量的各种方式，并了解如何查看它们的基本属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 导入 PyTorch (★☆☆)\n",
    "任务: 导入 torch 和 numpy 库，并打印各自的版本号，这是使用 PyTorch 的第一步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本: 2.7.0+cu118\n",
      "NumPy 版本: 1.26.3\n"
     ]
    }
   ],
   "source": [
    "# 导入 torch 和 numpy 库\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 打印导入成功信息和版本号\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 从 Python 列表与 NumPy 数组创建 (★☆☆)\n",
    "任务: 将一个 Python 列表 [[1, 2], [3, 4]] 和一个 NumPy 数组 np.array([[5, 6], [7, 8]]) 分别转换为 Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 从列表创建的 Tensor ---\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "--- 从 NumPy 创建的 Tensor ---\n",
      "tensor([[5, 6],\n",
      "        [7, 8]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 定义一个 Python 列表\n",
    "data = [[1, 2], [3, 4]]\n",
    "# 创建一个 NumPy 数组\n",
    "np_arr = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# 从列表创建\n",
    "X = torch.tensor(data)\n",
    "# 从 NumPy 数组创建 (注意：会共享内存)\n",
    "Y = torch.from_numpy(np_arr)\n",
    "\n",
    "print(\"--- 从列表创建的 Tensor ---\")\n",
    "print(X)\n",
    "print(\"\\n--- 从 NumPy 创建的 Tensor ---\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 创建常量 Tensor (★☆☆)\n",
    "任务: 创建三个形状均为 (2, 3) 的张量：一个全零张量，一个全一张量，以及一个所有元素均为常数 7 的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 全零 Tensor ---\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "--- 全一 Tensor ---\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "--- 填充为 7 的 Tensor ---\n",
      "tensor([[7, 7, 7],\n",
      "        [7, 7, 7]])\n"
     ]
    }
   ],
   "source": [
    "# 定义形状\n",
    "shape = (2, 3)\n",
    "\n",
    "# 创建全零、全一、全为7的张量\n",
    "t_zeros = torch.zeros(shape)\n",
    "t_ones = torch.ones(shape)\n",
    "t_sevens = torch.full(shape, 7)\n",
    "\n",
    "print(\"--- 全零 Tensor ---\")\n",
    "print(t_zeros)\n",
    "print(\"\\n--- 全一 Tensor ---\")\n",
    "print(t_ones)\n",
    "print(\"\\n--- 填充为 7 的 Tensor ---\")\n",
    "print(t_sevens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 获取 Tensor 的核心属性 (★☆☆)\n",
    "任务: 对于给定的张量 X = torch.tensor([[1.0, 2.0], [3.0, 4.0]])，一次性打印出它的形状 (shape)、数据类型 (dtype) 和所在设备 (device)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tensor 的核心属性 ---\n",
      "形状 (Shape): torch.Size([2, 2])\n",
      "数据类型 (dtype): torch.float32\n",
      "所在设备 (device): cpu\n"
     ]
    }
   ],
   "source": [
    "# 创建一个示例张量\n",
    "X = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# 获取并打印核心属性\n",
    "print(f\"--- Tensor 的核心属性 ---\")\n",
    "print(f\"形状 (Shape): {X.shape}\")\n",
    "print(f\"数据类型 (dtype): {X.dtype}\")\n",
    "print(f\"所在设备 (device): {X.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 创建指定数据类型的 Tensor (★☆☆)\n",
    "任务: 创建一个形状为 (2, 3)，数据类型为 torch.float16 (半精度浮点) 的全一 Tensor，并验证其数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 创建的 Tensor ---\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float16)\n",
      "\n",
      "--- 验证其数据类型 ---\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 定义形状和目标数据类型\n",
    "shape = (2, 3)\n",
    "dtype = torch.float16\n",
    "\n",
    "# 在创建时通过 dtype 参数指定数据类型\n",
    "Y = torch.ones(shape, dtype=dtype)\n",
    "\n",
    "print(f\"--- 创建的 Tensor ---\")\n",
    "print(Y)\n",
    "print(f\"\\n--- 验证其数据类型 ---\")\n",
    "print(Y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 仿照其他 Tensor 创建新 Tensor (★☆☆)\n",
    "任务: 给定一个已存在的源张量 src，它的值为 [[1, 2, 3], [4, 5, 6]] 且类型为 float16，请创建一个与它属性（形状、类型、设备）都相同，但值为全零的新 Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 源 Tensor ---\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], dtype=torch.float16)\n",
      "属性: torch.Size([2, 3]), torch.float16, cpu\n",
      "\n",
      "--- 仿照创建的全零 Tensor ---\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float16)\n",
      "属性: torch.Size([2, 3]), torch.float16, cpu\n"
     ]
    }
   ],
   "source": [
    "# 定义一个源张量\n",
    "src = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float16)\n",
    "\n",
    "# 使用 _like 函数可以方便地复制属性\n",
    "Z = torch.zeros_like(src)\n",
    "\n",
    "print(\"--- 源 Tensor ---\")\n",
    "print(src)\n",
    "print(f\"属性: {src.shape}, {src.dtype}, {src.device}\")\n",
    "print(\"\\n--- 仿照创建的全零 Tensor ---\")\n",
    "print(Z)\n",
    "print(f\"属性: {Z.shape}, {Z.dtype}, {Z.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 创建不同分布的随机 Tensor (★☆☆)\n",
    "任务: 创建两个形状均为 (3, 3) 的随机 Tensor：X 的元素在 [0, 1) 区间均匀分布，Y 的元素服从标准正态分布（均值为0，方差为1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [0, 1) 均匀分布随机 Tensor ---\n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408]])\n",
      "\n",
      "--- 标准正态分布随机 Tensor ---\n",
      "tensor([[ 1.5231,  0.6647, -1.0324],\n",
      "        [-0.2770, -0.1671, -0.1079],\n",
      "        [-1.4285, -0.2810,  0.7489]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以保证结果可复现\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 定义形状\n",
    "shape = (3, 3)\n",
    "\n",
    "# 创建不同分布的随机张量\n",
    "X = torch.rand(shape)  # 均匀分布\n",
    "Y = torch.randn(shape) # 标准正态分布\n",
    "\n",
    "print(f\"--- [0, 1) 均匀分布随机 Tensor ---\")\n",
    "print(X)\n",
    "print(f\"\\n--- 标准正态分布随机 Tensor ---\")\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 创建序列 Tensor (★☆☆)\n",
    "任务: 创建两个序列 Tensor：X 是一个从 0 到 10（不含10）、步长为1的整数序列；Y 是一个从 0 到 1（包含1）、均匀划分成5个元素的浮点数序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- arange 创建的整数序列 ---\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "--- linspace 创建的等差序列 ---\n",
      "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# 使用 arange 创建整数序列 (不含结束点)\n",
    "X = torch.arange(0, 10)\n",
    "\n",
    "# 使用 linspace 创建等差序列 (包含结束点)\n",
    "# 参数: start, end, steps\n",
    "Y = torch.linspace(0, 1, 5)\n",
    "\n",
    "print(\"--- arange 创建的整数序列 ---\")\n",
    "print(X)\n",
    "print(\"\\n--- linspace 创建的等差序列 ---\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 在指定设备上创建 Tensor (★☆☆)\n",
    "任务: 检查您的环境中 CUDA (GPU) 是否可用，如果可用，则在第一个 GPU (cuda:0) 上创建一个 2x2 的全一张量；否则，在 CPU 上创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 创建的 Tensor ---\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n",
      "它所在的设备是: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 检查 CUDA 是否可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 使用 device 参数在指定设备上创建张量\n",
    "Z = torch.ones(2, 2, device=device)\n",
    "\n",
    "print(f\"\\n--- 创建的 Tensor ---\")\n",
    "print(Z)\n",
    "print(f\"它所在的设备是: {Z.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块二：索引、切片与形状变换 🔪\n",
    "掌握了如何创建 Tensor 后，下一步是学习如何灵活地访问和重塑它们。这是数据预处理和特征工程中的核心技能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 基础索引：访问特定元素 (★☆☆)\n",
    "任务: 创建一个 3x3 的序列张量 X，其值为 0 到 8。然后，访问并打印出位于第 2 行、第 1 列的元素（注意索引从0开始）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor ---\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "\n",
      "--- 第2行、第1列的元素是 ---\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 3x3 的张量\n",
    "X = torch.arange(9).reshape(3, 3)\n",
    "\n",
    "# 访问第 2 行 (索引为1), 第 1 列 (索引为0) 的元素\n",
    "element = X[1, 0]\n",
    "\n",
    "print(\"--- 原始 Tensor ---\")\n",
    "print(X)\n",
    "print(f\"\\n--- 第2行、第1列的元素是 ---\")\n",
    "print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. 基础切片：提取整行与整列 (★☆☆)\n",
    "任务: 对于一个 4x4 的序列张量 Y（值为0-15），提取并打印出它的第 3 整行和第 4 整列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor ---\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "\n",
      "--- 第 3 行 ---\n",
      "tensor([ 8,  9, 10, 11])\n",
      "\n",
      "--- 第 4 列 ---\n",
      "tensor([ 3,  7, 11, 15])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 4x4 的张量\n",
    "Y = torch.arange(16).reshape(4, 4)\n",
    "\n",
    "# 提取第 3 行 (索引为2)\n",
    "third_row = Y[2, :] # 或者 Y[2]\n",
    "\n",
    "# 提取第 4 列 (索引为3)\n",
    "fourth_col = Y[:, 3]\n",
    "\n",
    "print(\"--- 原始 Tensor ---\")\n",
    "print(Y)\n",
    "print(\"\\n--- 第 3 行 ---\")\n",
    "print(third_row)\n",
    "print(\"\\n--- 第 4 列 ---\")\n",
    "print(fourth_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. 进阶切片：提取子矩阵 (★☆☆)\n",
    "任务: 对于一个 5x5 的序列张量 Z（值为0-24），提取出由第3、4行和第2、3、4列共同构成的 2x3 子矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor ---\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24]])\n",
      "\n",
      "--- 提取的 2x3 子矩阵 ---\n",
      "tensor([[11, 12, 13],\n",
      "        [16, 17, 18]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 5x5 的张量\n",
    "Z = torch.arange(25).reshape(5, 5)\n",
    "\n",
    "# 提取子矩阵：行索引为 2, 3；列索引为 1, 2, 3\n",
    "sub_matrix = Z[2:4, 1:4]\n",
    "\n",
    "print(\"--- 原始 Tensor ---\")\n",
    "print(Z)\n",
    "print(\"\\n--- 提取的 2x3 子矩阵 ---\")\n",
    "print(sub_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. 布尔索引：筛选满足条件的元素 (★☆☆)\n",
    "任务: 创建一个 4x4 的随机整数张量 X（值在0-9之间），筛选出所有大于5的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor ---\n",
      "tensor([[7, 5, 2, 7],\n",
      "        [2, 5, 7, 2],\n",
      "        [1, 5, 6, 3],\n",
      "        [1, 0, 6, 3]])\n",
      "\n",
      "--- 大于 5 的元素 ---\n",
      "tensor([7, 7, 7, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以保证结果可复现\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# 创建一个 4x4 的随机整数张量\n",
    "X = torch.randint(0, 10, (4, 4))\n",
    "\n",
    "# 创建布尔掩码 (Mask)\n",
    "mask = X > 5\n",
    "\n",
    "# 使用掩码进行索引，筛选出符合条件的元素\n",
    "filtered_elements = X[mask]\n",
    "\n",
    "print(\"--- 原始 Tensor ---\")\n",
    "print(X)\n",
    "print(\"\\n--- 大于 5 的元素 ---\")\n",
    "print(filtered_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. 使用布尔索引进行原地修改 (★☆☆)\n",
    "任务: 创建一个 3x4 的标准正态分布随机张量 Y，然后将其中所有小于0的元素都原地修改为0（这类似于ReLU激活函数的操作）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor ---\n",
      "tensor([[-1.2061,  0.0617,  1.1632, -1.5008],\n",
      "        [-1.5944, -0.0187, -2.1325, -0.5270],\n",
      "        [-0.1021,  0.0099, -0.4454, -1.4976]])\n",
      "\n",
      "--- 修改后的 Tensor ---\n",
      "tensor([[0.0000, 0.0617, 1.1632, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0099, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(20)\n",
    "\n",
    "# 创建一个 3x4 的随机张量\n",
    "Y = torch.randn(3, 4)\n",
    "print(\"--- 原始 Tensor ---\")\n",
    "print(Y)\n",
    "\n",
    "# 使用布尔索引找到小于0的元素，并原地赋值为0\n",
    "Y[Y < 0] = 0\n",
    "\n",
    "print(\"\\n--- 修改后的 Tensor ---\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. 改变形状 (★☆☆)\n",
    "任务: 创建一个值为 0-11 的一维张量 Z，然后将其形状变为 3x4。请分别使用两种不同的方法实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor ---\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "\n",
      "--- 使用 .view() 的结果 ---\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "--- 使用 .reshape() 的结果 ---\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个一维张量\n",
    "Z = torch.arange(12)\n",
    "\n",
    "# 使用 .view() 改变形状\n",
    "# .view() 要求原始张量的内存是连续的\n",
    "Z_view = Z.view(3, 4)\n",
    "\n",
    "# 使用 .reshape() 改变形状\n",
    "# .reshape() 更灵活，如果内存不连续，它会先创建副本再改变形状\n",
    "Z_reshape = Z.reshape(3, 4)\n",
    "\n",
    "print(\"--- 原始 Tensor ---\")\n",
    "print(Z)\n",
    "print(\"\\n--- 使用 .view() 的结果 ---\")\n",
    "print(Z_view)\n",
    "print(\"\\n--- 使用 .reshape() 的结果 ---\")\n",
    "print(Z_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. 增加维度 (★☆☆)\n",
    "任务: 创建一个形状为 (3, 4) 的张量 X。在第1个维度（dim=0）和第3个维度（dim=2）上增加一个新维度，使其最终形状变为 (1, 3, 1, 4)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor (shape: torch.Size([3, 4])) ---\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "--- 在 dim=0 unsqueeze 后 (shape: torch.Size([1, 3, 4])) ---\n",
      "--- 在 dim=2 unsqueeze 后 (shape: torch.Size([1, 3, 1, 4])) ---\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 3x4 的张量\n",
    "X = torch.arange(12).reshape(3, 4)\n",
    "print(f\"--- 原始 Tensor (shape: {X.shape}) ---\")\n",
    "print(X)\n",
    "\n",
    "# 在第 1 个维度 (dim=0) 增加维度\n",
    "X_unsqueezed_1 = X.unsqueeze(0)\n",
    "print(f\"\\n--- 在 dim=0 unsqueeze 后 (shape: {X_unsqueezed_1.shape}) ---\")\n",
    "\n",
    "# 在第 3 个维度 (dim=2) 增加维度\n",
    "X_unsqueezed_2 = X_unsqueezed_1.unsqueeze(2)\n",
    "print(f\"--- 在 dim=2 unsqueeze 后 (shape: {X_unsqueezed_2.shape}) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. 压缩维度 (★☆☆)\n",
    "任务: 创建一个形状为 (1, 3, 1, 2) 的随机张量 Y，移除所有大小为1的维度，使其最终形状变为 (3, 2)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor (shape: torch.Size([1, 3, 1, 2])) ---\n",
      "tensor([[[[0.4489, 0.2113]],\n",
      "\n",
      "         [[0.6839, 0.7478]],\n",
      "\n",
      "         [[0.4627, 0.7742]]]])\n",
      "\n",
      "--- squeeze 后的 Tensor (shape: torch.Size([3, 2])) ---\n",
      "tensor([[0.4489, 0.2113],\n",
      "        [0.6839, 0.7478],\n",
      "        [0.4627, 0.7742]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个包含冗余维度的张量\n",
    "Y = torch.rand(1, 3, 1, 2)\n",
    "\n",
    "# 使用 .squeeze() 移除所有大小为1的维度\n",
    "Y_squeezed = Y.squeeze()\n",
    "\n",
    "print(f\"--- 原始 Tensor (shape: {Y.shape}) ---\")\n",
    "print(Y)\n",
    "print(f\"\\n--- squeeze 后的 Tensor (shape: {Y_squeezed.shape}) ---\")\n",
    "print(Y_squeezed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. 交换维度 (★☆☆)\n",
    "任务: 在深度学习中，图像张量通常以 (C, H, W)（通道, 高, 宽）格式存储。请创建一个形状为 (3, 224, 224) 的模拟图像张量 Z，并将其维度转换为 (H, W, C)，以方便图像显示，可以尝试多种方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor 形状 (C, H, W) ---\n",
      "torch.Size([3, 224, 224])\n",
      "\n",
      "--- permute 后的 Tensor 形状 (H, W, C) ---\n",
      "torch.Size([224, 224, 3])\n",
      "\n",
      "--- transpose 后的 Tensor 形状 (H, W, C) ---\n",
      "torch.Size([224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个模拟图像张量 (通道, 高, 宽)\n",
    "C, H, W = 3, 224, 224\n",
    "Z = torch.randn(C, H, W)\n",
    "\n",
    "# 使用 .permute() 交换维度\n",
    "# 将 (0, 1, 2) -> (1, 2, 0)\n",
    "Z_permuted = Z.permute(1, 2, 0)\n",
    "\n",
    "# 使用 .transpose() 交换维度\n",
    "Z_transpose = Z.transpose(0, 1).transpose(1, 2)\n",
    "\n",
    "\n",
    "print(f\"--- 原始 Tensor 形状 (C, H, W) ---\")\n",
    "print(Z.shape)\n",
    "print(f\"\\n--- permute 后的 Tensor 形状 (H, W, C) ---\")\n",
    "print(Z_permuted.shape)\n",
    "print(f\"\\n--- transpose 后的 Tensor 形状 (H, W, C) ---\")\n",
    "print(Z_transpose.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19. 沿现有维度拼接张量 (★☆☆)\n",
    "任务: 创建两个形状均为 (2, 3) 的张量 X ∈ [0,6) 和 Y ∈ [6,12)。将它们分别沿维度0（行方向）和维度1（列方向）进行拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor X ---\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "--- 原始 Tensor Y ---\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "--- 沿 dim=0 拼接 (结果形状: 4x3) ---\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "--- 沿 dim=1 拼接 (结果形状: 2x6) ---\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "# 创建两个 2x3 的张量\n",
    "X = torch.arange(6).reshape(2, 3)\n",
    "Y = torch.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "# 沿维度0 (行) 拼接\n",
    "cat_dim0 = torch.cat((X, Y), dim=0)\n",
    "\n",
    "# 沿维度1 (列) 拼接\n",
    "cat_dim1 = torch.cat((X, Y), dim=1)\n",
    "\n",
    "print(\"--- 原始 Tensor X ---\")\n",
    "print(X)\n",
    "print(\"\\n--- 原始 Tensor Y ---\")\n",
    "print(Y)\n",
    "print(\"\\n--- 沿 dim=0 拼接 (结果形状: 4x3) ---\")\n",
    "print(cat_dim0)\n",
    "print(\"\\n--- 沿 dim=1 拼接 (结果形状: 2x6) ---\")\n",
    "print(cat_dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. 沿新维度堆叠张量 (★☆☆)\n",
    "任务: 创建两个形状均为 (2, 3) 的张量 X 和 Y。将它们堆叠起来，创建一个新的维度，使结果张量的形状变为 (2, 2, 3)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor X (shape: 2x3) ---\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "--- 原始 Tensor Y (shape: 2x3) ---\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "\n",
      "--- 堆叠后的 Tensor (结果形状: 2x2x3) ---\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "堆叠后形状: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 重新创建两个 2x3 的张量\n",
    "X = torch.arange(6).reshape(2, 3)\n",
    "Y = torch.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "# 沿一个新维度 (dim=0) 进行堆叠\n",
    "stacked_tensor = torch.stack((X, Y), dim=0)\n",
    "\n",
    "print(\"--- 原始 Tensor X (shape: 2x3) ---\")\n",
    "print(X)\n",
    "print(\"\\n--- 原始 Tensor Y (shape: 2x3) ---\")\n",
    "print(Y)\n",
    "print(\"\\n--- 堆叠后的 Tensor (结果形状: 2x2x3) ---\")\n",
    "print(stacked_tensor)\n",
    "print(f\"堆叠后形状: {stacked_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. 分割张量：精确控制与便捷分割 (★★☆)\n",
    "任务: 对一个7x4的张量Z，将其沿行（dim=0）均匀分割成2块（可以尝试两种方法），再将第一个块其沿行（dim=0）分割成不等大小的块[1,3]，PyTorch会自动处理不能整除的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 用于分割的原始 Tensor (7x4) ---\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19],\n",
      "        [20, 21, 22, 23],\n",
      "        [24, 25, 26, 27]])\n",
      "--- chunk 分割成的2个块 ---\n",
      "(tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]]), tensor([[16, 17, 18, 19],\n",
      "        [20, 21, 22, 23],\n",
      "        [24, 25, 26, 27]]))\n",
      "--- split 分割成的2个块 ---\n",
      "(tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]]), tensor([[16, 17, 18, 19],\n",
      "        [20, 21, 22, 23],\n",
      "        [24, 25, 26, 27]]))\n",
      "--- split 分割成不等大小的块 (分别为1, 3行) ---\n",
      "(tensor([[0, 1, 2, 3]]), tensor([[ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]]))\n"
     ]
    }
   ],
   "source": [
    "Z = torch.arange(28).reshape(7, 4)\n",
    "print(\"--- 用于分割的原始 Tensor (7x4) ---\")\n",
    "print(Z)\n",
    "\n",
    "# 将张量沿 dim=0 分割成大小分别为2块\n",
    "chunks = torch.chunk(Z, 2, dim=0)\n",
    "print(\"--- chunk 分割成的2个块 ---\")\n",
    "print(chunks)\n",
    "\n",
    "# 将张量沿 dim=0 分割成大小分别为2块(这里的4指的是每块大小为4)\n",
    "splits = torch.split(Z, 4, dim=0)\n",
    "print(\"--- split 分割成的2个块 ---\")\n",
    "print(splits)\n",
    "\n",
    "# 将第一个块沿 dim=0 分割成不等大小的块\n",
    "unequal_chunks = torch.split(chunks[0], [1, 3], dim=0)\n",
    "print(\"--- split 分割成不等大小的块 (分别为1, 3行) ---\")\n",
    "print(unequal_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. 综合运用：重塑与拼接 (★★☆)\n",
    "任务: 假设你有一个批次的数据 (Batch, Features)，形状为 (4, 10)。你需要将其分成两组，每组2个样本，分别进行不同的处理（这里可以用乘以不同常数模拟×2 or ×3），然后再将处理后的结果合并起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始批次数据 (4x10) ---\n",
      "torch.Size([4, 10])\n",
      "tensor([[7, 5, 2, 7, 2, 5, 7, 2, 1, 5],\n",
      "        [6, 3, 1, 0, 6, 3, 4, 0, 6, 2],\n",
      "        [8, 9, 2, 0, 9, 9, 4, 4, 9, 4],\n",
      "        [4, 5, 4, 0, 8, 9, 3, 0, 9, 3]])\n",
      "\n",
      "--- 分割后的两组 (均为 2x10) ---\n",
      "组1形状: torch.Size([2, 10]), 组2形状: torch.Size([2, 10])\n",
      "tensor([[7, 5, 2, 7, 2, 5, 7, 2, 1, 5],\n",
      "        [6, 3, 1, 0, 6, 3, 4, 0, 6, 2]])\n",
      "tensor([[8, 9, 2, 0, 9, 9, 4, 4, 9, 4],\n",
      "        [4, 5, 4, 0, 8, 9, 3, 0, 9, 3]])\n",
      "\n",
      "--- 重新拼接后的最终批次 (4x10) ---\n",
      "torch.Size([4, 10])\n",
      "tensor([[14, 10,  4, 14,  4, 10, 14,  4,  2, 10],\n",
      "        [12,  6,  2,  0, 12,  6,  8,  0, 12,  4],\n",
      "        [24, 27,  6,  0, 27, 27, 12, 12, 27, 12],\n",
      "        [12, 15, 12,  0, 24, 27,  9,  0, 27,  9]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "# 创建一个模拟的批次数据\n",
    "batch_data = torch.randint(0, 10, (4, 10))\n",
    "\n",
    "# 1. 将数据分割成两组\n",
    "group1, group2 = torch.chunk(batch_data, 2, dim=0)\n",
    "\n",
    "# 2. 模拟对两组数据进行不同的处理\n",
    "processed_group1 = group1 * 2\n",
    "processed_group2 = group2 * 3\n",
    "\n",
    "# 3. 将处理后的结果重新拼接成一个批次\n",
    "final_batch = torch.cat((processed_group1, processed_group2), dim=0)\n",
    "\n",
    "print(\"--- 原始批次数据 (4x10) ---\")\n",
    "print(batch_data.shape)\n",
    "print(batch_data)\n",
    "print(\"\\n--- 分割后的两组 (均为 2x10) ---\")\n",
    "print(f\"组1形状: {group1.shape}, 组2形状: {group2.shape}\")\n",
    "print(group1)\n",
    "print(group2)\n",
    "print(\"\\n--- 重新拼接后的最终批次 (4x10) ---\")\n",
    "print(final_batch.shape)\n",
    "print(final_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23. 高级索引：按指定索引筛选 (★★☆)\n",
    "任务: 创建一个 5x4 的张量 X（值为0-19）。现在，你有一个包含行索引的 1D 张量tensor([0, 3, 1, 3])。请从 X 中提取出这些索引对应的行，并组成新的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor (5x4) ---\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "\n",
      "--- 指定的行索引 ---\n",
      "tensor([0, 3, 1, 3])\n",
      "\n",
      "--- index_select 提取的结果 ---\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [12, 13, 14, 15],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 5x4 的张量\n",
    "X = torch.arange(20).reshape(5, 4)\n",
    "# 定义要提取的行索引\n",
    "indices = torch.tensor([0, 3, 1, 3])\n",
    "\n",
    "# 使用 index_select 沿维度 0 (行) 提取\n",
    "selected_rows = torch.index_select(X, dim=0, index=indices)\n",
    "\n",
    "print(\"--- 原始 Tensor (5x4) ---\")\n",
    "print(X)\n",
    "print(\"\\n--- 指定的行索引 ---\")\n",
    "print(indices)\n",
    "print(\"\\n--- index_select 提取的结果 ---\")\n",
    "print(selected_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24. 高级索引：按指定索引收集 (★★★)\n",
    "任务: 给定一个 3x5 的随机int张量 Y，以及一个 3x2 的索引张量 indices。我们需要从 Y 中有选择地提取元素，具体规则是：indices 的每一行包含2个数字，指定从 Y 对应行中应该选择哪些列（例如，如果 indices[0] = [4, 1]，则从 Y 的第 0 行中选择第 4 列和第 1 列的元素），最终结果应该是一个形状为 3x2 的新张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor Y (3x5) ---\n",
      "tensor([[67, 77, 77, 13, 66],\n",
      "        [34, 89, 72, 27, 37],\n",
      "        [45, 76, 61, 45, 59]])\n",
      "\n",
      "--- 要收集的列索引 (3x2) ---\n",
      "tensor([[4, 1],\n",
      "        [0, 2],\n",
      "        [3, 3]])\n",
      "\n",
      "--- 按索引选择的结果 (3x2) ---\n",
      "tensor([[66, 77],\n",
      "        [34, 72],\n",
      "        [45, 45]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以保证结果可复现\n",
    "torch.manual_seed(30)\n",
    "\n",
    "# 创建一个 3x5 的随机张量\n",
    "Y = torch.randint(10, 100, (3, 5))\n",
    "# 第0行收集第4、1列；第1行收集第0、2列；第2行收集第3、3列\n",
    "indices = torch.tensor([[4, 1], [0, 2], [3, 3]])\n",
    "\n",
    "# 按照索引从原始张量中提取特定元素\n",
    "# 输出形状与索引张量的形状相同\n",
    "gathered_elements = torch.gather(Y, dim=1, index=indices)\n",
    "\n",
    "print(\"--- 原始 Tensor Y (3x5) ---\")\n",
    "print(Y)\n",
    "print(\"\\n--- 要收集的列索引 (3x2) ---\")\n",
    "print(indices)\n",
    "print(\"\\n--- 按索引选择的结果 (3x2) ---\")\n",
    "print(gathered_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25. 条件元素替换与形状保持(★★☆)\n",
    "任务: 创建一个 4x4 的随机整数张量 Z (值在 -5 到 5 之间)。对 Z 中的每个元素应用条件判断，如果元素值大于 0，则保留原始值；如果元素值小于等于 0，则将其替换为 0，输出结果应当保持与输入张量完全相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor Z ---\n",
      "tensor([[ 2,  4,  3,  2],\n",
      "        [-4,  3,  2,  1],\n",
      "        [-2,  3,  0, -2],\n",
      "        [ 1, -3,  2, -2]])\n",
      "\n",
      "--- 使用 where 处理后的 Tensor ---\n",
      "tensor([[2, 4, 3, 2],\n",
      "        [0, 3, 2, 1],\n",
      "        [0, 3, 0, 0],\n",
      "        [1, 0, 2, 0]])\n",
      "\n",
      "--- 使用布尔索引处理后的 Tensor ---\n",
      "tensor([[2, 4, 3, 2],\n",
      "        [0, 3, 2, 1],\n",
      "        [0, 3, 0, 0],\n",
      "        [1, 0, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(35)\n",
    "# 创建一个 4x4 的随机张量\n",
    "Z = torch.randint(-5, 5, (4, 4))\n",
    "\n",
    "# 使用 torch.where(condition, x, y)\n",
    "# 当条件为 True 时，取 x 的值；否则取 y 的值\n",
    "# 这里 y 我们用一个全零的标量 0 代替，它会自动广播\n",
    "result = torch.where(Z > 0, Z, 0)\n",
    "\n",
    "# 尝试使用布尔索引\n",
    "result_bool = torch.zeros_like(Z)\n",
    "result_bool[Z > 0] = Z[Z > 0] # 虽然布尔索引可以实现，但是不是原地操作\n",
    "\n",
    "print(\"--- 原始 Tensor Z ---\")\n",
    "print(Z)\n",
    "print(\"\\n--- 使用 where 处理后的 Tensor ---\")\n",
    "print(result)\n",
    "print(\"\\n--- 使用布尔索引处理后的 Tensor ---\")\n",
    "print(result_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. 内存布局：理解 contiguous (★★☆)\n",
    "任务: 创建一个 3x4 的张量 A，然后通过 transpose(0, 1) 将其变为 4x3。检查转置后的张量 A_t 的内存是否还是连续的 (is_contiguous())。尝试对 A_t 执行 .view(2, 6) 操作，观察是否会报错，并解释原因。最后，使用 .contiguous() 修复它，再成功地改变其形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 Tensor A 是连续的吗? True\n",
      "转置后的 Tensor A_t 是连续的吗? False\n",
      "\n",
      "--- 直接对 A_t 使用 .view() 失败！---\n",
      "错误信息: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "\n",
      "--- A_t.contiguous() 是连续的吗? True ---\n",
      "--- 成功 view 后的 Tensor (2x6) ---\n",
      "tensor([[ 0,  4,  8,  1,  5,  9],\n",
      "        [ 2,  6, 10,  3,  7, 11]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 3x4 张量\n",
    "A = torch.arange(12).reshape(3, 4)\n",
    "print(f\"原始 Tensor A 是连续的吗? {A.is_contiguous()}\")\n",
    "\n",
    "# 转置张量\n",
    "A_t = A.transpose(0, 1)\n",
    "print(f\"转置后的 Tensor A_t 是连续的吗? {A_t.is_contiguous()}\")\n",
    "\n",
    "# 尝试对非连续张量使用 .view()\n",
    "try:\n",
    "    A_t.view(2, 6)\n",
    "except RuntimeError as e:\n",
    "    print(\"\\n--- 直接对 A_t 使用 .view() 失败！---\")\n",
    "    print(f\"错误信息: {e}\")\n",
    "\n",
    "# 使用 .contiguous() 创建一个内存连续的副本，再进行 view\n",
    "A_contiguous = A_t.contiguous()\n",
    "print(f\"\\n--- A_t.contiguous() 是连续的吗? {A_contiguous.is_contiguous()} ---\")\n",
    "A_view = A_contiguous.view(2, 6)\n",
    "print(\"--- 成功 view 后的 Tensor (2x6) ---\")\n",
    "print(A_view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 27. 高精度定向填充 (★★☆)\n",
    "任务：创建一个形状为 4×5 的全零张量 B。你需要按以下要求精确地在特定位置填入值：\n",
    "- 在第 0 行的第 2 列填入 11\n",
    "- 在第 1 行的第 0 列填入 22\n",
    "- 在第 3 行的第 4 列填入 33\n",
    "\n",
    "这个操作要求你能够根据给定的位置索引（行和列）高效地更新张量中的特定元素。\n",
    "\n",
    "提示：考虑如何通过索引列表同时定位并更新多个不同位置的值，尝试寻找一种比循环更高效的张量操作方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 方法一：直接索引赋值的结果 ---\n",
      "tensor([[ 0,  0, 11,  0,  0],\n",
      "        [22,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0, 33]])\n",
      "\n",
      "--- 方法二：使用 index_put_ 操作的结果 ---\n",
      "tensor([[ 0,  0, 11,  0,  0],\n",
      "        [22,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0, 33]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 4x5 的全零张量\n",
    "B = torch.zeros(4, 5, dtype=torch.int64)\n",
    "\n",
    "# 方法一：直接索引赋值\n",
    "B[0, 2] = 11  # 在第 0 行的第 2 列填入 11\n",
    "B[1, 0] = 22  # 在第 1 行的第 0 列填入 22\n",
    "B[3, 4] = 33  # 在第 3 行的第 4 列填入 33\n",
    "\n",
    "print(\"--- 方法一：直接索引赋值的结果 ---\")\n",
    "print(B)\n",
    "\n",
    "# 定义要填充的位置索引\n",
    "row_indices = torch.tensor([0, 1, 3])\n",
    "col_indices = torch.tensor([2, 0, 4])\n",
    "values = torch.tensor([11, 22, 33])\n",
    "\n",
    "# 方法二：使用 index_put_ 操作\n",
    "B_index_put = torch.zeros(4, 5, dtype=torch.int64)\n",
    "\n",
    "# 使用 index_put_ 方法一次性更新多个位置\n",
    "B_index_put.index_put_((row_indices, col_indices), values)\n",
    "\n",
    "print(\"\\n--- 方法二：使用 index_put_ 操作的结果 ---\")\n",
    "print(B_index_put)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28. 综合应用：掩码填充 (★★☆)\n",
    "任务: 创建一个 5x5 的序列张量 C（值为0-24）。创建一个布尔掩码 mask，其中所有大于 3 的元素位置为 True，并将 C 中对应 mask 为 True 的位置的值都改为 -1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor C ---\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24]])\n",
      "\n",
      "--- 布尔掩码 (值为 True 的位置将被填充) ---\n",
      "tensor([[False, False, False, False,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "\n",
      "--- masked_fill_ 后的结果 ---\n",
      "tensor([[ 0,  1,  2,  3, -1],\n",
      "        [-1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1]])\n",
      "\n",
      "--- 直接索引赋值的结果 ---\n",
      "tensor([[ 0,  1,  2,  3, -1],\n",
      "        [-1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 5x5 张量\n",
    "C = torch.arange(25).reshape(5, 5)\n",
    "\n",
    "# 创建布尔掩码\n",
    "mask = (C > 3)\n",
    "\n",
    "# 使用 masked_fill_ 进行原地填充\n",
    "# 注意函数名结尾的下划线 `_` 表示这是个原地 (in-place) 操作\n",
    "C.masked_fill_(mask, -1)\n",
    "\n",
    "print(\"--- 原始 Tensor C ---\")\n",
    "print(torch.arange(25).reshape(5, 5))\n",
    "print(\"\\n--- 布尔掩码 (值为 True 的位置将被填充) ---\")\n",
    "print(mask)\n",
    "print(\"\\n--- masked_fill_ 后的结果 ---\")\n",
    "print(C)\n",
    "\n",
    "# 直接使用索引赋值\n",
    "C = torch.arange(25).reshape(5, 5)\n",
    "C[C > 3] = -1\n",
    "print(\"\\n--- 直接索引赋值的结果 ---\")\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 29. 扩展与平铺：使用 expand 和 repeat (★★☆)\n",
    "任务: 创建一个形状为 (3, 1) 的张量 D。分别使用 .expand() 和 .repeat() 将其扩展为一个 3x4 的张量。比较两种方法，思考并探索它们在内存中的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始 Tensor D (3x1) ---\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n",
      "--- 使用 expand 后的 Tensor (3x4) ---\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [2, 2, 2, 2],\n",
      "        [3, 3, 3, 3]])\n",
      "\n",
      "--- 使用 repeat 后的 Tensor (3x4) ---\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [2, 2, 2, 2],\n",
      "        [3, 3, 3, 3]])\n",
      "\n",
      "expand 后的 storage 指针与原指针相同: True\n",
      "repeat 后的 storage 指针与原指针相同: False\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 (3, 1) 的张量\n",
    "D = torch.tensor([[1], [2], [3]])\n",
    "\n",
    "# 使用 .expand(-1,4)，-1 表示该维度的大小不改变\n",
    "# .expand() 不会分配新内存，只是创建了新的视图，更高效\n",
    "D_expanded = D.expand(-1, 4)\n",
    "\n",
    "# 使用 .repeat(1,4)，1表示该维度不变，4表示该维度重复4次\n",
    "# .repeat() 会复制数据，在内存中创建新的副本\n",
    "D_repeated = D.repeat(1, 4)\n",
    "\n",
    "print(\"--- 原始 Tensor D (3x1) ---\")\n",
    "print(D)\n",
    "print(\"\\n--- 使用 expand 后的 Tensor (3x4) ---\")\n",
    "print(D_expanded)\n",
    "print(\"\\n--- 使用 repeat 后的 Tensor (3x4) ---\")\n",
    "print(D_repeated)\n",
    "\n",
    "# 验证 expand 不分配新内存\n",
    "D_expanded_storage_ptr = D_expanded.untyped_storage().data_ptr()\n",
    "D_storage_ptr = D.untyped_storage().data_ptr()\n",
    "print(f\"\\nexpand 后的 storage 指针与原指针相同: {D_expanded_storage_ptr == D_storage_ptr}\")\n",
    "\n",
    "# 验证 repeat 分配了新内存\n",
    "D_repeated_storage_ptr = D_repeated.untyped_storage().data_ptr()\n",
    "print(f\"repeat 后的 storage 指针与原指针相同: {D_repeated_storage_ptr == D_storage_ptr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30. 综合应用：图像批次处理 (★★☆)\n",
    "任务: 假设你有一个批次的灰度图像数据，其形状为 (10, 1, 28, 28)，代表 10 张 28x28 的单通道图像。现在，你需要将其转换为 (10, 28, 28, 3)，模拟将灰度图转为 RGB 图（通过简单复制通道）。\n",
    "\n",
    "- 移除大小为 1 的通道维度。\n",
    "- 增加一个新的通道维度。\n",
    "- 将新的通道维度扩展为 3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: squeeze(1) 后形状: torch.Size([10, 28, 28])\n",
      "Step 2: unsqueeze(-1) 后形状: torch.Size([10, 28, 28, 1])\n",
      "Step 3: expand 后的最终形状: torch.Size([10, 28, 28, 3])\n"
     ]
    }
   ],
   "source": [
    "# 模拟一个批次的图像数据 (N, C, H, W)\n",
    "batch_images = torch.randn(10, 1, 28, 28)\n",
    "\n",
    "# 1. 移除大小为 1 的通道维度 -> (10, 28, 28)\n",
    "squeezed_batch = batch_images.squeeze(1)\n",
    "print(f\"Step 1: squeeze(1) 后形状: {squeezed_batch.shape}\")\n",
    "\n",
    "# 2. 在末尾增加一个新的通道维度 -> (10, 28, 28, 1)\n",
    "unsqueezed_batch = squeezed_batch.unsqueeze(-1)\n",
    "print(f\"Step 2: unsqueeze(-1) 后形状: {unsqueezed_batch.shape}\")\n",
    "\n",
    "# 3. 使用 expand 将最后一个维度扩展为 3 -> (10, 28, 28, 3)\n",
    "rgb_batch = unsqueezed_batch.expand(-1, -1, -1, 3)\n",
    "print(f\"Step 3: expand 后的最终形状: {rgb_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 31. 综合挑战：提取棋盘格点 (★★★)\n",
    "任务: 创建一个 8x8 的张量 E，代表一个棋盘，值为 0 到 63。你的任务是只提取出所有 \"白格\" 上的数字。假设左上角 (0, 0) 是白格，那么行索引和列索引之和为偶数的位置就是白格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始棋盘 (8x8) ---\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44, 45, 46, 47],\n",
      "        [48, 49, 50, 51, 52, 53, 54, 55],\n",
      "        [56, 57, 58, 59, 60, 61, 62, 63]])\n",
      "\n",
      "--- 白格掩码 ---\n",
      "tensor([[ True, False,  True, False,  True, False,  True, False],\n",
      "        [False,  True, False,  True, False,  True, False,  True],\n",
      "        [ True, False,  True, False,  True, False,  True, False],\n",
      "        [False,  True, False,  True, False,  True, False,  True],\n",
      "        [ True, False,  True, False,  True, False,  True, False],\n",
      "        [False,  True, False,  True, False,  True, False,  True],\n",
      "        [ True, False,  True, False,  True, False,  True, False],\n",
      "        [False,  True, False,  True, False,  True, False,  True]])\n",
      "\n",
      "--- 方法一提取出的所有白格元素 ---\n",
      "tensor([ 0,  2,  4,  6,  9, 11, 13, 15, 16, 18, 20, 22, 25, 27, 29, 31, 32, 34,\n",
      "        36, 38, 41, 43, 45, 47, 48, 50, 52, 54, 57, 59, 61, 63])\n",
      "共提取了 32 个元素\n",
      "--- 方法二计算得到的rows和cols ---\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [4, 4, 4, 4, 4, 4, 4, 4],\n",
      "        [5, 5, 5, 5, 5, 5, 5, 5],\n",
      "        [6, 6, 6, 6, 6, 6, 6, 6],\n",
      "        [7, 7, 7, 7, 7, 7, 7, 7]]) tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "\n",
      "--- 方法二提取出的所有白格元素 ---\n",
      "tensor([ 0,  2,  4,  6,  9, 11, 13, 15, 16, 18, 20, 22, 25, 27, 29, 31, 32, 34,\n",
      "        36, 38, 41, 43, 45, 47, 48, 50, 52, 54, 57, 59, 61, 63])\n",
      "共提取了 32 个元素\n"
     ]
    }
   ],
   "source": [
    "# --- 解法一 ---\n",
    "# 创建一个 8x8 的棋盘\n",
    "E = torch.arange(64).reshape(8, 8)\n",
    "\n",
    "# 创建行和列的索引网格\n",
    "rows = torch.arange(8).view(8, 1)\n",
    "cols = torch.arange(8).view(1, 8)\n",
    "\n",
    "# 计算每个位置的索引和（广播机制）\n",
    "index_sum = rows + cols\n",
    "\n",
    "# 创建布尔掩码，索引和为偶数的位置是 True\n",
    "mask = (index_sum % 2 == 0)\n",
    "\n",
    "# 使用布尔索引提取白格上的元素\n",
    "white_cells = E[mask]\n",
    "\n",
    "print(\"--- 原始棋盘 (8x8) ---\")\n",
    "print(E)\n",
    "print(\"\\n--- 白格掩码 ---\")\n",
    "print(mask)\n",
    "print(\"\\n--- 方法一提取出的所有白格元素 ---\")\n",
    "print(white_cells)\n",
    "print(f\"共提取了 {white_cells.numel()} 个元素\")\n",
    "\n",
    "\n",
    "# --- 解法二 ---\n",
    "rows, cols = torch.meshgrid(torch.arange(8), torch.arange(8), indexing='ij')\n",
    "white_cells = E[(rows + cols) % 2 == 0] # 无广播操作，因为rows和cols都是8x8的矩阵\n",
    "print(\"--- 方法二计算得到的rows和cols ---\")\n",
    "print(rows, cols)\n",
    "print(\"\\n--- 方法二提取出的所有白格元素 ---\")\n",
    "print(white_cells)\n",
    "print(f\"共提取了 {white_cells.numel()} 个元素\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32. 综合挑战：批量矩阵特定对角线提取 (★★★)\n",
    "任务: 假设你有一个形状为 (4, 5, 5) 的张量 F，代表一个批次的 4 个 5x5 矩阵。你需要提取出这 4 个矩阵的主对角线元素，最终得到一个形状为 (4, 5) 的张量。请思考如何高效完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始批次张量形状 ---\n",
      "torch.Size([4, 5, 5])\n",
      "\n",
      "--- 使用 torch.diagonal 提取的结果 (4, 5) ---\n",
      "tensor([[ 0,  6, 12, 18, 24],\n",
      "        [25, 31, 37, 43, 49],\n",
      "        [50, 56, 62, 68, 74],\n",
      "        [75, 81, 87, 93, 99]])\n",
      "\n",
      "--- 使用手动索引提取的结果 (4, 5) ---\n",
      "tensor([[ 0,  6, 12, 18, 24],\n",
      "        [25, 31, 37, 43, 49],\n",
      "        [50, 56, 62, 68, 74],\n",
      "        [75, 81, 87, 93, 99]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个批次的矩阵\n",
    "F = torch.arange(4 * 5 * 5).reshape(4, 5, 5)\n",
    "\n",
    "# 方法一：使用 gather\n",
    "# 我们需要一个索引张量，告诉 gather 去哪里取值\n",
    "# 对于第 k 个矩阵，我们要取 (k, 0, 0), (k, 1, 1), ..., (k, 4, 4)\n",
    "# 这很难用一次 gather 完成。\n",
    "\n",
    "\n",
    "# 方法二：使用 torch.diagonal(F, offset=0, dim1=1, dim2=2)\n",
    "# offset=0 表示主对角线，dim1和dim2构成了一个平面，函数在这个平面上取对角线\n",
    "# 这是最直接、最简洁的方法\n",
    "diagonals_direct = torch.diagonal(F, offset=0, dim1=1, dim2=2)\n",
    "\n",
    "\n",
    "# 方法三：手动构造索引 (更底层，有助于理解)\n",
    "# 创建一个范围 [0, 1, 2, 3, 4]\n",
    "indices = torch.arange(5)\n",
    "# F[:, indices, indices] 这种索引方式在 PyTorch 中可以直接使用\n",
    "diagonals_manual = F[:, indices, indices]\n",
    "\n",
    "print(\"--- 原始批次张量形状 ---\")\n",
    "print(F.shape)\n",
    "# print(\"\\n--- 使用 gather 提取的结果 (4, 5) ---\")\n",
    "# print(diagonals_gather)\n",
    "print(\"\\n--- 使用 torch.diagonal 提取的结果 (4, 5) ---\")\n",
    "print(diagonals_direct)\n",
    "print(\"\\n--- 使用手动索引提取的结果 (4, 5) ---\")\n",
    "print(diagonals_manual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33. 综合挑战：翻转序列 (★★☆)\n",
    "任务: 给定一个形状为 (5, 10) 的张量 G，代表 5 个长度为 10 的序列。请将每个序列独立地进行翻转，得到一个同样形状为 (5, 10) 的新张量，其中每一行都是原始行的逆序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这种写法在numpy中支持，但在PyTorch中不支持\n",
      "\n",
      "--- 原始序列批次 (5, 10) ---\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n",
      "\n",
      "--- 使用torch.flip()翻转每个序列后的结果 ---\n",
      "tensor([[ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n",
      "        [19, 18, 17, 16, 15, 14, 13, 12, 11, 10],\n",
      "        [29, 28, 27, 26, 25, 24, 23, 22, 21, 20],\n",
      "        [39, 38, 37, 36, 35, 34, 33, 32, 31, 30],\n",
      "        [49, 48, 47, 46, 45, 44, 43, 42, 41, 40]])\n",
      "\n",
      "--- 使用索引翻转每个序列后的结果 ---\n",
      "tensor([[ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n",
      "        [19, 18, 17, 16, 15, 14, 13, 12, 11, 10],\n",
      "        [29, 28, 27, 26, 25, 24, 23, 22, 21, 20],\n",
      "        [39, 38, 37, 36, 35, 34, 33, 32, 31, 30],\n",
      "        [49, 48, 47, 46, 45, 44, 43, 42, 41, 40]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个批次的序列\n",
    "G = torch.arange(50).reshape(5, 10)\n",
    "\n",
    "# --- 方法一 ---\n",
    "# 使用 `torch.flip()` 函数，指定要翻转的维度\n",
    "# dims控制反转的维度以及反转的顺序，[1]表示在第二个维度上反转\n",
    "G_flipped = torch.flip(G, dims=[1])\n",
    "\n",
    "# --- 方法二 ---\n",
    "# 使用索引反转每个序列\n",
    "G_flipped_index = G[:, torch.arange(G.size(1)-1, -1, -1)]\n",
    "\n",
    "# 也可以使用倒序列表作为索引下标，但是思路相同\n",
    "# index = [i for i in range(G.size(1)-1, -1, -1)]\n",
    "# G_flipped_index = G[:, index]\n",
    "\n",
    "# --- 方法三 ---\n",
    "# 使用切片反转每个序列\n",
    "# 语法 G[:, ::-1] 但在PyTorch中需要用专门的索引表示法\n",
    "try:\n",
    "    G_flipped_slice = G[:, ::-1]\n",
    "except:\n",
    "    print(\"这种写法在numpy中支持，但在PyTorch中不支持\\n\")\n",
    "\n",
    "print(\"--- 原始序列批次 (5, 10) ---\")\n",
    "print(G)\n",
    "print(\"\\n--- 使用torch.flip()翻转每个序列后的结果 ---\")\n",
    "print(G_flipped)\n",
    "print(\"\\n--- 使用索引翻转每个序列后的结果 ---\")\n",
    "print(G_flipped_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 34. 综合挑战：创建 One-Hot 编码 (★★★)\n",
    "任务: One-Hot 编码是分类任务中常用的数据预处理步骤。给定一个包含类别索引的一维张量 labels = torch.tensor([0, 2, 1, 4]) 和类别总数 num_classes = 5，请创建一个 4x5 的 One-Hot 编码张量。每一行对应一个标签，该行中标签索引的位置为 1，其余为 0。请尝试用 scatter_ 实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始标签 ---\n",
      "tensor([0, 2, 1, 4])\n",
      "\n",
      "--- 转换后的 One-Hot 编码张量 (4x5) ---\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 输入的标签和类别总数\n",
    "labels = torch.tensor([0, 2, 1, 4])\n",
    "num_classes = 5\n",
    "num_labels = labels.shape[0]\n",
    "\n",
    "# 1. 创建一个全零的目标张量 [4,5]\n",
    "one_hot = torch.zeros(num_labels, num_classes)\n",
    "\n",
    "# 2. 准备 scatter_ 需要的 index 张量\n",
    "# labels 需要是 (4, 1) 的形状，以指定每一行要修改的列索引\n",
    "index = labels.unsqueeze(1)\n",
    "\n",
    "# 3. 使用 scatter_ 填充\n",
    "# 在 dim=1 (列方向) 上操作\n",
    "# 对于 one_hot 的第 i 行，在 index[i] (也就是 labels[i]) 指定的列上，填入值 1\n",
    "one_hot.scatter_(dim=1, index=index, value=1.0)\n",
    "\n",
    "\n",
    "print(\"--- 原始标签 ---\")\n",
    "print(labels)\n",
    "print(\"\\n--- 转换后的 One-Hot 编码张量 (4x5) ---\")\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 35. 终极挑战：块交换 (★★★)\n",
    "任务: 创建一个 6x6 的张量 H。将其在两个维度上都均匀切分成 4 个 3x3 的块，然后将这 4 个块进行对角线交换（左上与右下交换，右上与左下交换）。\n",
    "\n",
    "原始布局: [[A, B], [C, D]]\n",
    "目标布局: [[D, C], [B, A]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 H (6x6) ---\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35]])\n",
      "\n",
      "--- 左上块 A ---\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 6,  7,  8],\n",
      "        [12, 13, 14]])\n",
      "--- 右上块 B ---\n",
      "tensor([[ 3,  4,  5],\n",
      "        [ 9, 10, 11],\n",
      "        [15, 16, 17]])\n",
      "--- 左下块 C ---\n",
      "tensor([[18, 19, 20],\n",
      "        [24, 25, 26],\n",
      "        [30, 31, 32]])\n",
      "--- 右下块 D ---\n",
      "tensor([[21, 22, 23],\n",
      "        [27, 28, 29],\n",
      "        [33, 34, 35]])\n",
      "\n",
      "--- 交换块后的最终结果 ---\n",
      "tensor([[21, 22, 23, 18, 19, 20],\n",
      "        [27, 28, 29, 24, 25, 26],\n",
      "        [33, 34, 35, 30, 31, 32],\n",
      "        [ 3,  4,  5,  0,  1,  2],\n",
      "        [ 9, 10, 11,  6,  7,  8],\n",
      "        [15, 16, 17, 12, 13, 14]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个 6x6 张量\n",
    "H = torch.arange(36).reshape(6, 6)\n",
    "\n",
    "# 1. 将 H 分割成 4 个 3x3 的块\n",
    "# 先沿行分割\n",
    "H_top, H_bottom = torch.chunk(H, 2, dim=0)\n",
    "# 再对上下两部分分别沿列分割\n",
    "A, B = torch.chunk(H_top, 2, dim=1)\n",
    "C, D = torch.chunk(H_bottom, 2, dim=1)\n",
    "\n",
    "# 2. 按照新的布局重新拼接\n",
    "# 创建新的顶行 [D, C] 和底行 [B, A]\n",
    "new_top_row = torch.cat((D, C), dim=1)\n",
    "new_bottom_row = torch.cat((B, A), dim=1)\n",
    "\n",
    "# 将新的两行沿行方向拼接\n",
    "result = torch.cat((new_top_row, new_bottom_row), dim=0)\n",
    "\n",
    "print(\"--- 原始张量 H (6x6) ---\")\n",
    "print(H)\n",
    "print(\"\\n--- 左上块 A ---\")\n",
    "print(A)\n",
    "print(\"--- 右上块 B ---\")\n",
    "print(B)\n",
    "print(\"--- 左下块 C ---\")\n",
    "print(C)\n",
    "print(\"--- 右下块 D ---\")\n",
    "print(D)\n",
    "print(\"\\n--- 交换块后的最终结果 ---\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模块三：数学与逻辑运算 🧮\n",
    "掌握张量的数值计算，这是构建神经网络前向传播的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 36. 基础运算与广播 (★★☆)\n",
    "任务: 创建一个形状为 (4, 1) 的张量 A 和一个形状为 (1, 3) 的张量 B。\n",
    "\n",
    "计算 A 和 B 的和得到张量 C ，并解释结果的形状为何是 (4, 3)。\n",
    "计算 C 中各元素的自然指数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 A (4x1) ---\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n",
      "--- 原始张量 B (1x3) ---\n",
      "tensor([[0, 1, 2]])\n",
      "\n",
      "--- A + B 的计算结果 (4x3) ---\n",
      "tensor([[0, 1, 2],\n",
      "        [1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5]])\n",
      "结果形状: torch.Size([4, 3])\n",
      "\n",
      "--- C 的自然指数结果 ---\n",
      "tensor([[  1.0000,   2.7183,   7.3891],\n",
      "        [  2.7183,   7.3891,  20.0855],\n",
      "        [  7.3891,  20.0855,  54.5981],\n",
      "        [ 20.0855,  54.5981, 148.4132]])\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建张量\n",
    "A = torch.arange(4).view(4, 1)\n",
    "B = torch.arange(3).view(1, 3)\n",
    "print(\"--- 原始张量 A (4x1) ---\")\n",
    "print(A)\n",
    "print(\"\\n--- 原始张量 B (1x3) ---\")\n",
    "print(B)\n",
    "\n",
    "# 2. 计算和，触发广播机制\n",
    "# A 的形状是 (4, 1)，B 的形状是 (1, 3)\n",
    "# 广播后，A 变为 (4, 3)，B 变为 (4, 3)，然后逐元素相加\n",
    "C = A + B\n",
    "print(\"\\n--- A + B 的计算结果 (4x3) ---\")\n",
    "print(C)\n",
    "print(f\"结果形状: {C.shape}\")\n",
    "\n",
    "# 3. 计算自然指数\n",
    "C_exp = torch.exp(C.float()) # exp需要浮点数输入\n",
    "print(\"\\n--- C 的自然指数结果 ---\")\n",
    "print(C_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 37. 元素处理组合 (★★☆)\n",
    "任务: 创建一个在 (-10, 10) 区间内均匀分布的 (3, 3) 随机张量 X。\n",
    "\n",
    "- 计算其所有元素的绝对值。\n",
    "- 将原始张量 X 的所有元素值限制在 [-5, 5] 区间内，超出范围的值赋值为区间边缘值（-5或5）。\n",
    "- 对限制范围后的结果进行四舍五入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 X ---\n",
      "tensor([[-0.8383, -0.3429, -3.7500],\n",
      "        [ 2.3004, -5.7211, -1.7635],\n",
      "        [ 3.8759,  9.3862,  2.3559]])\n",
      "\n",
      "--- X 的绝对值 ---\n",
      "tensor([[0.8383, 0.3429, 3.7500],\n",
      "        [2.3004, 5.7211, 1.7635],\n",
      "        [3.8759, 9.3862, 2.3559]])\n",
      "\n",
      "--- 将 X 限制在 [-5, 5] 内的结果 ---\n",
      "tensor([[-0.8383, -0.3429, -3.7500],\n",
      "        [ 2.3004, -5.0000, -1.7635],\n",
      "        [ 3.8759,  5.0000,  2.3559]])\n",
      "\n",
      "--- 四舍五入后的结果 ---\n",
      "X_rounded: tensor([[-1., -0., -4.],\n",
      "        [ 2., -5., -2.],\n",
      "        [ 4.,  5.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以复现结果\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# 1. 创建张量\n",
    "X = (torch.rand(3, 3) * 20) - 10 # 转换为 (-10, 10) 范围\n",
    "print(\"--- 原始张量 X ---\")\n",
    "print(X)\n",
    "\n",
    "# 2. 计算绝对值\n",
    "X_abs = torch.abs(X)\n",
    "print(\"\\n--- X 的绝对值 ---\")\n",
    "print(X_abs)\n",
    "\n",
    "# 3. 限制元素范围\n",
    "X_clamped = torch.clamp(X, min=-5, max=5)\n",
    "print(\"\\n--- 将 X 限制在 [-5, 5] 内的结果 ---\")\n",
    "print(X_clamped)\n",
    "\n",
    "# 4. 对结果进行四舍五入\n",
    "X_rounded = torch.round(X_clamped)\n",
    "print(\"\\n--- 四舍五入后的结果 ---\")\n",
    "print(\"X_rounded:\",X_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 38. 矩阵乘法的核心区别 (★★☆)\n",
    "任务: 创建两个形状均为 (2, 3) 的随机张量 A 和 B。\n",
    "\n",
    "- 计算 A 与 B 的逐元素乘积。\n",
    "- 计算 A 与 B 的转置（B.T）的矩阵乘积。\n",
    "- 解释两种乘法在计算方式和结果形状上的根本不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 A (2x3) ---\n",
      "tensor([[-1.2061,  0.0617,  1.1632],\n",
      "        [-1.5008, -1.5944, -0.0187]])\n",
      "\n",
      "--- 原始张量 B (2x3) ---\n",
      "tensor([[-2.1325, -0.5270, -0.1021],\n",
      "        [ 0.0099, -0.4454, -1.4976]])\n",
      "\n",
      "--- 逐元素乘积 (A * B) 的结果 (2x3) ---\n",
      "tensor([[ 2.5719, -0.0325, -0.1188],\n",
      "        [-0.0149,  0.7102,  0.0280]])\n",
      "\n",
      "--- 矩阵乘积的结果 (2x2) ---\n",
      "tensor([[ 2.4207, -1.7813],\n",
      "        [ 4.0427,  0.7234]])\n",
      "tensor([[ 2.4207, -1.7813],\n",
      "        [ 4.0427,  0.7234]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(20)\n",
    "\n",
    "# 1. 创建张量\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(2, 3)\n",
    "print(\"--- 原始张量 A (2x3) ---\")\n",
    "print(A)\n",
    "print(\"\\n--- 原始张量 B (2x3) ---\")\n",
    "print(B)\n",
    "\n",
    "# 2. 逐元素乘法\n",
    "# 要求两个张量形状相同，结果张量形状也与之相同\n",
    "element_wise_prod = A * B\n",
    "print(\"\\n--- 逐元素乘积 (A * B) 的结果 (2x3) ---\")\n",
    "print(element_wise_prod)\n",
    "\n",
    "# 3. 矩阵乘法\n",
    "# 要求第一个张量的列数等于第二个张量的行数\n",
    "# A(2x3) @ B.T(3x2) -> 结果为 (2x2)\n",
    "matrix_prod = A @ B.T\n",
    "matrix_prod_2 = torch.matmul(A, B.T)\n",
    "print(\"\\n--- 矩阵乘积的结果 (2x2) ---\")\n",
    "print(matrix_prod)\n",
    "print(matrix_prod_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 39. 矩阵向量点积运算 (★★☆)\n",
    "任务: 创建一个 (4, 2) 的矩阵 M 和一个长度为 2 的向量 V。\n",
    "\n",
    "- 计算矩阵与向量的乘积。\n",
    "- 提取 M 的第一行 row1，并计算 row1 与向量 V 的点积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 矩阵 M (4x2) ---\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.],\n",
      "        [6., 7.]])\n",
      "\n",
      "--- 向量 V (长度为2) ---\n",
      "tensor([2., 3.])\n",
      "\n",
      "--- 矩阵-向量乘积结果1 (长度为4) ---\n",
      "tensor([ 3., 13., 23., 33.])\n",
      "\n",
      "--- 矩阵-向量乘积结果2 (长度为4) ---\n",
      "tensor([ 3., 13., 23., 33.])\n",
      "\n",
      "--- 矩阵-向量乘积结果3 (长度为4) ---\n",
      "tensor([ 3., 13., 23., 33.])\n",
      "\n",
      "--- M的第一行 ([0. 1.]) 与 V 的点积结果 ---\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建张量\n",
    "M = torch.arange(8).float().reshape(4, 2)\n",
    "V = torch.tensor([2.0, 3.0])\n",
    "print(\"--- 矩阵 M (4x2) ---\")\n",
    "print(M)\n",
    "print(\"\\n--- 向量 V (长度为2) ---\")\n",
    "print(V)\n",
    "\n",
    "# 2. 计算矩阵-向量乘积\n",
    "# 结果是一个长度为4的向量\n",
    "# 注意：不论V向量是否转置，PyTorch都可以自动处理\n",
    "mv_prod = torch.mv(M, V)\n",
    "print(\"\\n--- 矩阵-向量乘积结果1 (长度为4) ---\")\n",
    "print(mv_prod)\n",
    "\n",
    "mv_prod_2 = M @ V\n",
    "print(\"\\n--- 矩阵-向量乘积结果2 (长度为4) ---\")\n",
    "print(mv_prod_2)\n",
    "\n",
    "mv_prod_3 = torch.matmul(M, V)\n",
    "print(\"\\n--- 矩阵-向量乘积结果3 (长度为4) ---\")\n",
    "print(mv_prod_3)\n",
    "\n",
    "\n",
    "# 3. 计算点积\n",
    "row1 = M[0, :]\n",
    "dot_prod = torch.dot(row1, V)\n",
    "print(f\"\\n--- M的第一行 ({row1.numpy()}) 与 V 的点积结果 ---\")\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 40. 三角函数应用 (★★☆)\n",
    "任务: 创建一个从 0 到 2π 均匀间隔的10个点的一维张量 theta。\n",
    "\n",
    "- 计算 theta 所有元素的正弦和余弦值。\n",
    "- 验证 \"正弦值的平方\" 与 \"余弦值的平方\" 之和，其结果张量中所有元素都近似为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 角度张量 theta ---\n",
      "tensor([0.0000, 0.6981, 1.3963, 2.0944, 2.7925, 3.4907, 4.1888, 4.8869, 5.5851,\n",
      "        6.2832])\n",
      "\n",
      "--- 正弦值 ---\n",
      "tensor([ 0.0000e+00,  6.4279e-01,  9.8481e-01,  8.6603e-01,  3.4202e-01,\n",
      "        -3.4202e-01, -8.6603e-01, -9.8481e-01, -6.4279e-01,  1.7485e-07])\n",
      "\n",
      "--- 余弦值 ---\n",
      "tensor([ 1.0000,  0.7660,  0.1736, -0.5000, -0.9397, -0.9397, -0.5000,  0.1736,\n",
      "         0.7660,  1.0000])\n",
      "\n",
      "--- sin^2 + cos^2 的结果 (应近似为1) ---\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建角度张量\n",
    "# 与numpy不同，torch.linspace()默认包含结束点\n",
    "theta = torch.linspace(0, 2 * torch.pi, 10)\n",
    "print(\"--- 角度张量 theta ---\")\n",
    "print(theta)\n",
    "\n",
    "# 2. 计算正弦和余弦\n",
    "sin_theta = torch.sin(theta)\n",
    "cos_theta = torch.cos(theta)\n",
    "print(\"\\n--- 正弦值 ---\")\n",
    "print(sin_theta)\n",
    "print(\"\\n--- 余弦值 ---\")\n",
    "print(cos_theta)\n",
    "\n",
    "# 3. 验证 sin^2 + cos^2 = 1\n",
    "sum_of_squares = sin_theta.pow(2) + cos_theta.pow(2)\n",
    "print(\"\\n--- sin^2 + cos^2 的结果 (应近似为1) ---\")\n",
    "print(sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 41. 基础聚合操作 (★★☆)\n",
    "任务: 创建一个 (5, 4) 的随机整数张量，计算整个张量的 总和、均值 和 标准差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 X (5x4) ---\n",
      "tensor([[17., 17., 17., 13.],\n",
      "        [16.,  4., 19.,  2.],\n",
      "        [17.,  7.,  5.,  6.],\n",
      "        [11.,  5., 19.,  7.],\n",
      "        [ 9.,  9.,  5.,  9.]])\n",
      "\n",
      "--- 聚合计算结果 ---\n",
      "总和: 214.00\n",
      "均值: 10.70\n",
      "标准差: 5.65\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(30)\n",
    "\n",
    "# 1. 创建张量\n",
    "X = torch.randint(0, 20, (5, 4)).float() # 转换为浮点数以便计算均值和标准差\n",
    "print(\"--- 原始张量 X (5x4) ---\")\n",
    "print(X)\n",
    "\n",
    "# 2. 计算聚合值\n",
    "total_sum = X.sum()\n",
    "mean_val = X.mean()\n",
    "std_val = X.std()\n",
    "\n",
    "\n",
    "print(f\"\\n--- 聚合计算结果 ---\")\n",
    "print(f\"总和: {total_sum.item():.2f}\")\n",
    "print(f\"均值: {mean_val.item():.2f}\")\n",
    "print(f\"标准差: {std_val.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 42. 按维度聚合与 keepdim (★★☆)\n",
    "任务: 对于一个 (5, 4) 的随机张量，分别计算 每一列 的 **总和** 和 每一行 的 **均值**。其中，在计算行均值时，请设置参数以保持结果的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 X (5x4) ---\n",
      "tensor([[ 0.1838,  0.1832, -2.3538, -0.1817],\n",
      "        [-0.2994, -2.1561,  0.4940,  2.4608],\n",
      "        [-1.2028,  0.8062,  1.2355, -0.5087],\n",
      "        [-2.5162,  0.1458,  1.5299,  0.9691],\n",
      "        [-0.7181, -1.6961,  1.0695, -0.9444]])\n",
      "\n",
      "--- 每一列的总和 (dim=0) ---\n",
      "tensor([-4.5527, -2.7169,  1.9752,  1.7952])\n",
      "结果形状: torch.Size([4])\n",
      "\n",
      "--- 每一行的均值 (dim=1, keepdim=True) ---\n",
      "tensor([[-0.5421],\n",
      "        [ 0.1248],\n",
      "        [ 0.0826],\n",
      "        [ 0.0322],\n",
      "        [-0.5723]])\n",
      "结果形状: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(35)\n",
    "X = torch.randn(5, 4)\n",
    "print(\"--- 原始张量 X (5x4) ---\")\n",
    "print(X)\n",
    "\n",
    "# 1. 计算每一列的总和 (沿 dim=0 聚合)\n",
    "# 结果形状为 (4,)\n",
    "col_sum = X.sum(dim=0)\n",
    "print(\"\\n--- 每一列的总和 (dim=0) ---\")\n",
    "print(col_sum)\n",
    "print(f\"结果形状: {col_sum.shape}\")\n",
    "\n",
    "# 2. 计算每一行的均值，并保持维度\n",
    "# 结果形状应为 (5, 1) 而不是 (5,)\n",
    "row_mean_keepdim = X.mean(dim=1, keepdim=True)\n",
    "print(\"\\n--- 每一行的均值 (dim=1, keepdim=True) ---\")\n",
    "print(row_mean_keepdim)\n",
    "print(f\"结果形状: {row_mean_keepdim.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 43. 查找最值及其索引 (★★☆)\n",
    "任务: 创建一个 (3, 4) 的随机张量。\n",
    "\n",
    "- 找出整个张量的最大值。\n",
    "- 找出每一行中的最大值及其对应的列索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 X (3x4) ---\n",
      "tensor([[ 0.9307, -0.3482,  0.8855, -0.4909],\n",
      "        [-0.4273,  0.9385,  1.1841,  0.5234],\n",
      "        [ 0.5403, -0.8986,  1.0984, -0.2616]])\n",
      "\n",
      "--- 全局最大值 ---\n",
      "1.1840680837631226\n",
      "\n",
      "--- 每一行的最大值 ---\n",
      "tensor([0.9307, 1.1841, 1.0984])\n",
      "\n",
      "--- 每一行最大值对应的列索引 ---\n",
      "tensor([0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(40)\n",
    "X = torch.randn(3, 4)\n",
    "print(\"--- 原始张量 X (3x4) ---\")\n",
    "print(X)\n",
    "\n",
    "# 1. 找出全局最大值\n",
    "global_max = X.max()\n",
    "print(f\"\\n--- 全局最大值 ---\")\n",
    "print(global_max.item())\n",
    "\n",
    "# 2. 找出每一行的最大值及其索引\n",
    "# 函数会返回一个包含(值, 索引)的元组\n",
    "row_max_values, row_max_indices = X.max(dim=1)\n",
    "print(\"\\n--- 每一行的最大值 ---\")\n",
    "print(row_max_values)\n",
    "print(\"\\n--- 每一行最大值对应的列索引 ---\")\n",
    "print(row_max_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 44. 向量与矩阵范数 (★★☆)\n",
    "任务: 创建一个 (2, 3) 的张量 X，计算其 L1范数 (所有元素的绝对值之和) 和 L2范数 (所有元素的平方和再开方，也称Frobenius范数)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 X (2x3) ---\n",
      "tensor([[-3., -2., -1.],\n",
      "        [ 0.,  1.,  2.]])\n",
      "\n",
      "--- L1 范数 ---\n",
      "tensor(9.)\n",
      "\n",
      "--- L2 范数 ---\n",
      "tensor(4.3589)\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建张量\n",
    "X = torch.arange(-3, 3).view(2, 3).float()\n",
    "print(\"--- 原始张量 X (2x3) ---\")\n",
    "print(X)\n",
    "\n",
    "# 2. 计算L1范数\n",
    "l1_norm = torch.norm(X, p=1)\n",
    "print(f\"\\n--- L1 范数 ---\")\n",
    "print(l1_norm)\n",
    "\n",
    "# 3. 计算L2范数 (Frobenius Norm)\n",
    "l2_norm = torch.norm(X, p=2)\n",
    "print(f\"\\n--- L2 范数 ---\")\n",
    "print(l2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45. 累积运算 (★★☆)\n",
    "任务: 创建一个一维整数张量 T，并计算其 累积和 与 累积积。\n",
    "- 累积和：沿着指定维度，计算到当前位置为止的所有元素之和\n",
    "例如对于张量 [1,2,3,4,5]\n",
    "累积和为 [1, 1+2, 1+2+3, 1+2+3+4, 1+2+3+4+5] = [1, 3, 6, 10, 15]\n",
    "- 累积积：沿着指定维度，计算到当前位置为止的所有元素之积\n",
    "例如对于张量 [1,2,3,4,5]\n",
    "累积积为 [1, 1×2, 1×2×3, 1×2×3×4, 1×2×3×4×5] = [1, 2, 6, 24, 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 创建张量\n",
    "T = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"--- 原始张量 T ---\")\n",
    "print(T)\n",
    "\n",
    "# 2. 计算累积和\n",
    "T_cumsum = torch.cumsum(T, dim=0)\n",
    "print(\"\\n--- T 的累积和 ---\")\n",
    "print(T_cumsum)\n",
    "\n",
    "# 3. 计算累积积\n",
    "T_cumprod = torch.cumprod(T, dim=0)\n",
    "print(\"\\n--- T 的累积积 ---\")\n",
    "print(T_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 46. 张量比较与逻辑聚合 (★★☆)\n",
    "任务: 创建两个 (3, 3) 的随机整数张量 A 和 B (范围0-9)。\n",
    "\n",
    "- 生成一个布尔张量，表示 A 中大于 B 的位置。\n",
    "- 判断 A 是否所有元素都大于0，以及 B 是否至少有一个元素等于7。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始张量 A ---\n",
      "tensor([[4, 8, 3],\n",
      "        [1, 9, 0],\n",
      "        [4, 0, 5]])\n",
      "\n",
      "--- 原始张量 B ---\n",
      "tensor([[4, 8, 7],\n",
      "        [5, 0, 7],\n",
      "        [0, 9, 1]])\n",
      "\n",
      "--- A > B 的布尔掩码 ---\n",
      "tensor([[False, False, False],\n",
      "        [False,  True, False],\n",
      "        [ True, False,  True]])\n",
      "\n",
      "--- 逻辑判断结果 ---\n",
      "A 的所有元素都大于0吗? False\n",
      "B 中至少有一个元素等于7吗? True\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(50)\n",
    "A = torch.randint(0, 10, (3, 3))\n",
    "B = torch.randint(0, 10, (3, 3))\n",
    "print(\"--- 原始张量 A ---\")\n",
    "print(A)\n",
    "print(\"\\n--- 原始张量 B ---\")\n",
    "print(B)\n",
    "\n",
    "# 1. 生成布尔掩码\n",
    "mask = A > B\n",
    "print(\"\\n--- A > B 的布尔掩码 ---\")\n",
    "print(mask)\n",
    "\n",
    "# 2. 逻辑聚合\n",
    "all_A_positive = torch.all(A > 0)\n",
    "any_B_is_7 = torch.any(B == 7)\n",
    "print(f\"\\n--- 逻辑判断结果 ---\")\n",
    "print(f\"A 的所有元素都大于0吗? {all_A_positive.item()}\")\n",
    "print(f\"B 中至少有一个元素等于7吗? {any_B_is_7.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 47. 条件选择 (★★☆)\n",
    "任务: 创建一个 (4, 4) 的标准正态分布随机张量。使用一个操作，将所有大于0.5的元素替换为1，小于-0.5的元素替换为-1，其余元素保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(60)\n",
    "X = torch.randn(4, 4)\n",
    "print(\"--- 原始张量 X ---\")\n",
    "print(X)\n",
    "\n",
    "# 1. 嵌套条件选择\n",
    "# where(condition, x, y): 当条件为True时取x，否则取y\n",
    "# 先将 > 0.5 的替换为 1\n",
    "result = torch.where(X > 0.5, 1.0, X)\n",
    "# 再对上一步结果中 < -0.5 的替换为 -1\n",
    "result = torch.where(result < -0.5, -1.0, result)\n",
    "\n",
    "print(\"\\n--- 条件替换后的结果 ---\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 48. 频率统计 (★★★)\n",
    "任务: 创建一个包含 [0, 1, 2, 1, 2, 2, 0, 4, 2] 的一维整数张量 labels，高效地统计每个数字（类别）出现的次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始标签张量 ---\n",
      "tensor([0, 1, 2, 1, 2, 2, 0, 4, 2])\n",
      "\n",
      "--- 各类别出现次数统计 ---\n",
      "tensor([2, 2, 4, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建标签张量\n",
    "labels = torch.tensor([0, 1, 2, 1, 2, 2, 0, 4, 2])\n",
    "print(\"--- 原始标签张量 ---\")\n",
    "print(labels)\n",
    "\n",
    "# 2. 统计频率\n",
    "# 结果张量的索引代表类别，值代表该类别的数量\n",
    "# minlength确保即使最大标签4不存在，结果张量的长度也至少为5\n",
    "counts = torch.bincount(labels, minlength=5)\n",
    "print(\"\\n--- 各类别出现次数统计 ---\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 49. 综合应用：数据标准化 (★★★)\n",
    "任务: 创建一个 (10, 5) 的随机张量 D，并将第2列所有元素赋值为5。对 D 的每一列（特征）进行 Z-Score 标准化（使其均值为0，标准差为1）。请注意处理标准差为零的边缘情况（此时该列所有元素应变为0）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始数据 D ---\n",
      "tensor([[ 0.8271, -0.8417,  5.0000, -0.2040,  1.4320],\n",
      "        [-0.5107,  1.5798,  5.0000,  0.6767, -0.4259],\n",
      "        [-0.5820,  0.7566,  5.0000,  0.1647,  0.2752],\n",
      "        [-0.1702, -0.6664,  5.0000, -1.2822,  0.7640],\n",
      "        [ 0.9887,  1.7393,  5.0000, -1.5190,  0.6253],\n",
      "        [ 0.4719,  1.4954,  5.0000,  2.4252, -0.5634],\n",
      "        [ 0.5487,  1.2654,  5.0000,  0.4422, -0.5691],\n",
      "        [-0.1377,  1.6001,  5.0000, -1.5182, -0.6699],\n",
      "        [ 0.5538,  0.0637,  5.0000, -1.2409,  0.1662],\n",
      "        [-0.7296,  0.5355,  5.0000,  0.0094,  0.4708]])\n",
      "\n",
      "--- 每列的均值 ---\n",
      "tensor([ 0.1260,  0.7528,  5.0000, -0.2046,  0.1505])\n",
      "\n",
      "--- 每列的标准差 ---\n",
      "tensor([0.6248, 0.9585, 0.0000, 1.2464, 0.6988])\n",
      "\n",
      "--- 标准化后的数据 ---\n",
      "tensor([[ 1.1221e+00, -1.6636e+00,  0.0000e+00,  5.0551e-04,  1.8338e+00],\n",
      "        [-1.0191e+00,  8.6289e-01,  0.0000e+00,  7.0708e-01, -8.2480e-01],\n",
      "        [-1.1331e+00,  4.0492e-03,  0.0000e+00,  2.9629e-01,  1.7840e-01],\n",
      "        [-4.7402e-01, -1.4806e+00,  0.0000e+00, -8.6459e-01,  8.7785e-01],\n",
      "        [ 1.3809e+00,  1.0293e+00,  0.0000e+00, -1.0545e+00,  6.7940e-01],\n",
      "        [ 5.5361e-01,  7.7476e-01,  0.0000e+00,  2.1099e+00, -1.0216e+00],\n",
      "        [ 6.7654e-01,  5.3486e-01,  0.0000e+00,  5.1898e-01, -1.0298e+00],\n",
      "        [-4.2209e-01,  8.8401e-01,  0.0000e+00, -1.0539e+00, -1.1741e+00],\n",
      "        [ 6.8472e-01, -7.1895e-01,  0.0000e+00, -8.3144e-01,  2.2502e-02],\n",
      "        [-1.3695e+00, -2.2671e-01,  0.0000e+00,  1.7168e-01,  4.5830e-01]])\n",
      "\n",
      "--- 验证标准化后数据的均值和标准差（应接近0和1）---\n",
      "新均值: tensor([ 1.1921e-08, -5.9605e-08,  0.0000e+00,  3.5763e-08, -4.7684e-08])\n",
      "新标准差: tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "torch.manual_seed(70)\n",
    "D = torch.randn(10, 5)\n",
    "# 人为制造一个标准差为0的列\n",
    "D[:, 2] = 5.0\n",
    "print(\"--- 原始数据 D ---\")\n",
    "print(D)\n",
    "\n",
    "# 1. 计算均值和标准差\n",
    "mean_D = D.mean(dim=0)\n",
    "std_D = D.std(dim=0)\n",
    "print(\"\\n--- 每列的均值 ---\")\n",
    "print(mean_D)\n",
    "print(\"\\n--- 每列的标准差 ---\")\n",
    "print(std_D)\n",
    "\n",
    "# 2. 进行标准化\n",
    "# 增加一个极小值 epsilon 防止除以零\n",
    "epsilon = 1e-8\n",
    "D_normalized = (D - mean_D) / (std_D + epsilon)\n",
    "\n",
    "# 检查标准差为0的列是否正确处理（或者直接将其设为0）\n",
    "# D_normalized[:, std_D == 0] = 0 # 更稳健的做法\n",
    "\n",
    "print(\"\\n--- 标准化后的数据 ---\")\n",
    "print(D_normalized)\n",
    "print(\"\\n--- 验证标准化后数据的均值和标准差（应接近0和1）---\")\n",
    "print(\"新均值:\", D_normalized.mean(dim=0))\n",
    "print(\"新标准差:\", D_normalized.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50. 终极挑战：模拟Vision Transformer的图像分块与分析 (★★★)\n",
    "- 背景: Vision Transformer (ViT) 是一种在图像识别领域取得巨大成功的模型。它颠覆了传统卷积网络（CNN）的思路，将图像分割成一系列固定大小的“图块”(Patches)，然后将这些图块的序列输入到Transformer模型中进行学习。这个挑战将模拟ViT模型中最关键、最巧妙的第一步：图像分块 (Image Patching)，并对分块结果进行简单的分析。\n",
    "\n",
    "- 任务: 给定一个模拟的 32x32 RGB图像，你需要不使用任何Python循环，仅通过张量操作将其分割成 4x4 大小的图块，并找出“最亮”的那个图块。\n",
    "\n",
    "- 具体步骤:\n",
    "\n",
    "1. 图像分块 (张量塑形):\n",
    "   - 创建一个 (3, 32, 32) 的随机张量 image 来模拟一张RGB图像。\n",
    "   - 定义 patch_size = 4。\n",
    "   - 通过一系列的塑形和维度变换操作，将 image 张量转换成一个形状为 (64, 3, 4, 4) 的 patches 张量。这代表了 8x8=64 个图块，每个图块都是 3x4x4 的。\n",
    "2. 计算各块的平均颜色 (聚合运算):\n",
    "   - 对上一步得到的 patches 张量进行聚合运算，计算出64个图块中，每一个图块的R、G、B三个通道的颜色均值。\n",
    "   - 最终应得到一个形状为 (64, 3) 的张量 mean_colors。\n",
    "3. 寻找最亮的图块 (数学运算与查找):\n",
    "   - “亮度”可以被近似定义为颜色向量的强度。请计算 mean_colors 张量中每一个图块的颜色向量（长度为3）的 L2范数。\n",
    "   - 找出L2范数最大的那个图块，并返回它在64个图块中的 索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 任务设定 ---\n",
      "原始图像形状: torch.Size([3, 32, 32])\n",
      "图块大小: 4x4\n",
      "\n",
      "--- 1. 图像分块结果 ---\n",
      "分块后张量形状: torch.Size([64, 3, 4, 4])\n",
      "共得到 64 个 4x4 的图块。\n",
      "\n",
      "--- 2. 各图块的平均颜色 ---\n",
      "平均颜色张量形状: torch.Size([64, 3])\n",
      "前5个图块的平均R,G,B值:\n",
      "tensor([[140.5000, 120.0000, 112.1250],\n",
      "        [ 86.5000, 108.6250, 154.4375],\n",
      "        [145.4375, 106.0625, 142.2500],\n",
      "        [123.7500, 125.8750, 122.8750],\n",
      "        [130.2500, 114.1875,  94.1875]])\n",
      "\n",
      "--- 3. 寻找最亮的图块 ---\n",
      "每个图块的亮度(L2范数):\n",
      "tensor([216.1302, 207.6839, 229.4262, 215.0740, 197.1678, 264.0987, 214.4895,\n",
      "        237.5283, 210.7780, 227.2983, 222.4846, 256.0205, 273.3630, 228.7893,\n",
      "        230.9097, 196.8072, 195.3281, 212.8864, 231.6634, 206.6679, 235.8289,\n",
      "        259.5774, 207.3605, 227.7825, 204.3258, 194.4541, 216.9226, 195.8058,\n",
      "        201.0884, 240.1360, 229.5229, 207.2234, 220.3880, 207.2952, 228.2402,\n",
      "        245.2392, 211.3525, 191.8025, 250.3566, 230.7233, 224.6295, 228.2636,\n",
      "        228.7965, 203.7335, 198.0699, 242.1788, 259.8570, 230.5771, 207.8853,\n",
      "        210.4995, 255.8450, 208.7542, 230.8481, 247.0584, 226.0972, 239.1875,\n",
      "        225.8618, 193.7221, 187.6368, 245.0456, 217.1308, 192.1367, 226.2981,\n",
      "        203.4558])\n",
      "\n",
      "最亮的图块索引是: 12\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以保证结果可复现\n",
    "torch.manual_seed(88)\n",
    "\n",
    "# 创建一个模拟的 32x32 RGB图像，值范围在0-255之间\n",
    "image = torch.randint(0, 256, (3, 32, 32), dtype=torch.float)\n",
    "patch_size = 4\n",
    "num_patches_per_dim = image.shape[1] // patch_size\n",
    "\n",
    "print(f\"--- 任务设定 ---\")\n",
    "print(f\"原始图像形状: {image.shape}\")\n",
    "print(f\"图块大小: {patch_size}x{patch_size}\")\n",
    "\n",
    "# --- 1. 图像分块 (核心步骤) ---\n",
    "# 这是一个非常巧妙的、无需循环的张量操作技巧\n",
    "# a. 将H和W维度分割成 (num_patches, patch_size)\n",
    "# (3, 32, 32) -> (3, 8*8, 4, 4)\n",
    "patches_temp = image.view(\n",
    "    3,\n",
    "    num_patches_per_dim**2,\n",
    "    patch_size,\n",
    "    patch_size\n",
    ")\n",
    "# b. 交换维度\n",
    "patches = patches_temp.transpose(0, 1)\n",
    "\n",
    "print(\"\\n--- 1. 图像分块结果 ---\")\n",
    "print(f\"分块后张量形状: {patches.shape}\")\n",
    "print(f\"共得到 {patches.shape[0]} 个 {patches.shape[2]}x{patches.shape[3]} 的图块。\")\n",
    "\n",
    "\n",
    "# --- 2. 计算各块的平均颜色 ---\n",
    "# 对每个图块的 H 和 W 维度求均值\n",
    "# 输入: (64, 3, 4, 4) -> 输出: (64, 3)\n",
    "mean_colors = patches.mean(dim=(-1, -2)) # -1和-2代表最后两个维度(H, W)\n",
    "\n",
    "print(\"\\n--- 2. 各图块的平均颜色 ---\")\n",
    "print(f\"平均颜色张量形状: {mean_colors.shape}\")\n",
    "print(\"前5个图块的平均R,G,B值:\")\n",
    "print(mean_colors[:5, :])\n",
    "\n",
    "# --- 3. 寻找最亮的图块 ---\n",
    "# 计算每个颜色向量的L2范数\n",
    "# 输入: (64, 3) -> 输出: (64,)\n",
    "brightness = torch.norm(mean_colors, p=2, dim=1)\n",
    "# 找到范数最大值的索引\n",
    "brightest_patch_index = torch.argmax(brightness)\n",
    "\n",
    "print(\"\\n--- 3. 寻找最亮的图块 ---\")\n",
    "print(f\"每个图块的亮度(L2范数):\")\n",
    "print(brightness)\n",
    "print(f\"\\n最亮的图块索引是: {brightest_patch_index.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
